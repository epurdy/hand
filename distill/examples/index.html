<!--
    Copyright 2018 The Distill Template Authors

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
  -->
<!doctype html>

<head>
  <link rel=stylesheet href="codemirror/lib/codemirror.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
  <script src="template.v2.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  <script src="https://d3js.org/d3.v5.js"></script>
  <script src="codemirror/lib/codemirror.js"></script>
  <script src="codemirror/mode/yaml/yaml.js"></script>
  <script src="codemirror/keymap/emacs.js"></script>
  <script src="codemirror/addon/edit/matchbrackets.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/js-yaml/3.13.1/js-yaml.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/6.6.0/math.min.js"></script>
  <script src="hand_transformer.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf8">

  <style>
    .CodeMirror { border: 1px solid #ddd; }
    .CodeMirror-scroll { max-height: 500px; }
    .CodeMirror pre { padding-left: 7px; line-height: 1.25; }
  </style>

</head>

<body>
  <distill-header></distill-header>
  <d-front-matter>
    <script id='distill-front-matter' type="text/json">
      {
          "title": "Programmable Transformers",
          "description": "We can set the weights of a transformer by hand.",
          "published": "Feb 20, 2020",
          "authors": [
              {
                  "author": "Eric Purdy",
                  "authorURL": "https://ericpurdy.com/",
                  "affiliations": [{"name": "Rad AI"}]
              }
          ],
          "katex": {
              "delimiters": [
                  {"left": "$$", "right": "$$", "display": false}
              ]
          }
      }
    </script>
  </d-front-matter>
  <d-title>
    <h1>Programmable Transformers</h1>
    <figure style="grid-column: page; margin: 1rem 0;"><img src="header_img.png"
                                                            style="width:100%; border: 1px solid rgba(0, 0, 0, 0.2);" /></figure>
    <p>We propose to hardcode the parameters of a Transformer network
      in a new human-interpretable notation.</p>
  </d-title>
  <d-byline></d-byline>
  <d-article>



    <a class="marker" href="#section-1" id="section-1"><span>1</span></a>
    <h2>Introduction</h2>
    <p>Deep learning is very effective at creating networks that can
      perform complex tasks, but we do not generally have much
      idea <b>how</b> those tasks are performed by these networks.  We
      here propose some new notation that makes it far more possible
      to <b>hardcode</b> neural networks that can perform the same
      sorts of tasks as their learned counterparts. This has several
      benefits:
      <ul>
        <li>The resulting hardcoded networks are <b>extremely
            interpretable</b> and may be useful for some tasks where
          that is a requirement.
        </li>
        <li>The process of hardcoding a network provides valuable
          insight into <b>what kinds of computation are
            performable</b> with the various components of a particular
          architecture.
        </li>
        <li>Hardcoding a network gives a better understanding of the
          <b>space of possible variants</b> of a particular
          architecture.
        </li>
        <li>Hardcoding neural networks for natural language processing
          allows us to encode linguistic knowledge in a format that is
          usable for linguistic competence, thus allowing us
          to <b>rigorously test linguistic theories</b> and
          potentially providing <b>a new experimental framework for
            work in linguistics</b>.
        </li>
        <li>There are several ways to <b>combine hardcoded components
            with learned components</b> to achieve some of the
          advantages of both.
        </li>
      </ul>
    </p>

    <p>
      Our goal in this article is to successfully hardcode the weights
      of a Transformer<d-cite key="transformer"></d-cite> in order to
      do classification (is this sentence grammatical or not?) and
      translation (English to French). We present these networks
      piece-by-piece with editable code blocks so that the reader can
      follow along.  While we intend to make this article
      self-contained, readers may find it helpful to consult the
      Illustrated
      Transformer<d-cite key="illustrated-transformer"></d-cite> and
      the Annotated
      Transformer<d-cite key="annotated-transformer"></d-cite> to
      refresh their memories of how Transformer works.
    </p>



    <a class="marker" href="#section-2" id="section-2"><span>2</span></a>
    <h2>Notation</h2>
    <p>Our first bit of notation is for sparse vectors whose nonzero
      entries are small numbers. We first pick an ordered set of
      natural language words (e.g., English words), one per dimension
      of our vector space. We will call these
      words <b>semes</b>.<d-footnote>The term comes from semiotics,
        where it denotes the smallest piece of semantic
        meaning.</d-footnote> Each seme represents a different basis
      vector. We then represent vectors as sums of these basis
      vectors. If our semes are "pig", "peregrine", and "wombat", we
      would represent the vector $$\langle 1, 0, -1\rangle$$ as
      $$\langle \langle +pig - wombat \rangle \rangle$$, while the
      vector $$\langle -2.1, 3.3, 0\rangle$$ would be represented as
      $$\langle\langle -2.1 pig + 3.3 peregrine \rangle \rangle$$.
    </p>
    <p>When writing code, we omit the $$\langle\langle\, \rangle\rangle$$:
      <d-code block="" language="markdown">
        vec1: +pig -wombat
        vec2: -2.1pig +3.3peregrine
      </d-code>
    </p>
    <p>The next bit of notation is for sparse matrices whose nonzero
      entries are small numbers. We represent the $$i,j$$ entry of a
      matrix being $$x\ne 0$$ by $$\{\{ x \, seme_i \rightarrow
      seme_j\}\}$$. If our three semes are again "pig",
      "peregrine", and "wombat", then we have
      <d-math block>
        \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & 3\\ 0 & 0 & 4 \end{bmatrix}
        = \{\{ pig\rightarrow pig,
        2 peregrine\rightarrow peregrine,
        3 peregrine \rightarrow wombat,
        4 wombat\rightarrow wombat\}\}
      </d-math>
      <d-math block>
        \begin{bmatrix} 0 & 0 & 3 \\ -1 & 2 & -4\\ 0 & 0 & 0
        \end{bmatrix} = \{\{ 3pig\rightarrow wombat,
        -peregrine\rightarrow pig, 2peregrine
        \rightarrow peregrine, -4peregrine \rightarrow
        wombat\}\}
      </d-math>
      and so on.
    </p>
    <p>When writing code, we omit the $$\{\{ \}\}$$ and use $$>$$ in
      place of $$\rightarrow$$:
      <d-code block="" language="markdown">
        mat1: pig>pig, 2peregrine>peregrine, 3peregrine>wombat,
        4wombat>wombat
        mat2: 3pig>wombat, -peregrine>pig, 2peregrine>peregrine,
        -4peregrine>wombat
      </d-code>
    </p>



    <a class="marker" href="#section-3" id="section-3"><span>3</span></a>
    <h2>Discussion</h2>



    <a class="marker" href="#section-3.1" id="section-3.1"><span>3.1</span></a>
    <h2>Interpretability</h2>

    <p>It is generally accepted that deep neural networks are
      uninterpretable. We show in this article that they can be made
      quite interpretable by using better notation, with no or very
      little change to the architecture. Unfortunately, building a
      network that is able to perform a nontrivial task is quite
      complex and requires either a lot of preliminaries or intimate
      familiarity with the architectures in question; we will
      gradually build up to such networks. To whet the reader&apos;s
      appetite, we show here a simple (vanilla) RNN designed to do
      sentiment analysis, based on the VADER
      algorithm.<d-cite key="vader"></d-cite> (VADER itself is already
      extremely interpretable; this is only meant to demonstrate that
      a fairly simple vanilla RNN can be made just as interpretable.)
    </p>

    <p>
      The following figure can be edited interactively. We have
      provided a bare minimum set of word embeddings (below called
      "lexicon"), so adding additional examples will probably require
      adding additional items to the lexicon.
    </p>

    <aside>Any shortcomings of the network below should not be taken
      to reflect on the VADER algorithm, of which this network is only
      a crude sketch.
    </aside>

    <d-figure id="figure-vader" style="border: 1px solid black">
      <svg id="figure-vader-svg" width="500" height="200" />
    </d-figure>
    <script>
      const figureVader = document.querySelector(
          "d-figure#figure-vader");
      const initTagVader = document.createElement("div");
      figureVader.appendChild(initTagVader);

      let layer = null;

      figureVader.addEventListener("ready", function () {
          const initTagVader = figureVader.querySelector("div");
          layer = new HandRnn(
              {program: `
semes: stop positive negative negation contrastive intensifier
       lessener intensepunctuation
       xa xb xc xd xe ya yb yc yd ye

lexicon:
    ",": stop
    ".": stop
    "I'll": stop
    At: stop
    It: stop
    The: stop
    Today: stop
    VADER: stop
    a: stop
    all: stop
    and: stop
    are: stop
    at: stop
    book: stop
    by: stop
    characters: stop
    dialog: stop
    get: stop
    is: stop
    it: stop
    of: stop
    plot: stop
    the: stop
    was: stop
    FUNNY: positive
    GOOD: positive
    GREAT: positive
    HANDSOME: positive
    LOL: positive
    SMART: positive
    funny: positive
    good: positive
    great: positive
    handsome: positive
    lol: positive
    smart: positive
    SUX: negative
    bad: negative
    horrible: negative
    sux: negative
    uncompelling: negative
    "!": intensepunctuation
    "!!!": intensepunctuation
    very: intensifier
    VERY: intensifier
    uber: intensifier
    FRIGGIN: intensifier
    only: lessener
    kinda: lessener
    not: negation
    nor: stop
    isnt: negation
    Not: negation
    But: contrastive
    but: contrastive

# H^0_t = I_t
# H^l_0 = 0
# H^l_t = sigmoid[(I + A^l) H^{l-1}_t + B^l H^l_{t-1} + bias^l]
# X^0 = mean_t H^2_t
# X^1 = sigmoid(C^1 X^0 + c^l)

rnn_layer1:
    A: positive>xa negative>ya positive>xb negative>yb
       negation>negation intensifier>intensifier lessener>lessener

    B: intensifier>xa intensifier>ya lessener>xb lessener>yb
       negation>negation 0.5intensifier>intensifier 0.5lessener>lessener

    bias: -xa -xb -ya -yb

rnn_layer2:
    A: xa>xc ya>yc xb>xd yb>yd negation>xc negation>xd negation>yc
       negation>yd positive>xe negation>xe negative>ye negation>ye

    B: ''

    bias: -xc -xd -yc -yd -xe -ye

dense1:
    C: positive>positive negative>negative 2xa>positive 0.5xb>positive
       ya>negative 0.5yb>negative xc>negative xd>negative yc>positive
       yd>positive -2xc>positive -2xd>positive -2yc>negative
       -2yd>negative xe>negative ye>positive -xe>positive -ye>negative
    c: ''


examples:  # examples modified from https://github.com/cjhutto/vaderSentiment
    - VADER is smart , handsome , and funny .
    - VADER is smart , handsome , and funny !
    - VADER is very smart , handsome , and funny .
    - VADER is VERY SMART , handsome , and FUNNY .
    - VADER is VERY SMART , handsome , and FUNNY !!!
    - VADER is VERY SMART , uber handsome , and FRIGGIN FUNNY !!!
    - VADER is not smart , handsome , nor funny .
    - The book was good .
    - It isnt a horrible book .
    - The book was only kinda good .
    - The plot was good , but the characters are uncompelling and the dialog is not great .
    - Today SUX !
    - Today only kinda sux ! But I'll get by , lol
    - Not bad at all
`,
               id: 'figure-vader',
               topLevel: true,
               document: document,
               container: initTagVader});
      });

    </script>



    <a class="marker" href="#section-3.2" id="section-3.2"><span>3.2</span></a>
    <h2>As a Tool to Build Intuition</h2>

    <aside>
    <li>The process of hardcoding a network provides valuable
          insight into <b>what kinds of computation are
            performable</b> with the various components of a particular
          architecture.
        </li>
        <li>Hardcoding a network gives a better understanding of the
          <b>space of possible variants</b> of a particular
          architecture.
        </li>
    </aside>


    <a class="marker" href="#section-3.3" id="section-3.3"><span>3.3</span></a>
    <h2>Usefulness for Linguistics</h2>
        <aside>Hardcoding neural networks for natural language processing
          allows us to encode linguistic knowledge in a format that is
          usable for linguistic competence, thus allowing us
          to <b>rigorously test linguistic theories</b> and
          potentially providing <b>a new experimental framework for
            work in linguistics</b>.
        </aside>


    <a class="marker" href="#section-3.4" id="section-3.4"><span>3.4</span></a>
    <h2>Combining Learning with Programming</h2>
        <aside>There are several ways to <b>combine hardcoded components
            with learned components</b> to achieve some of the
          advantages of both.
        </aside>



    <a class="marker" href="#section-4" id="section-4"><span>4</span></a>
    <h2>Ingredients of Transformer Networks</h2>

    <p>In this section, we lay out the workings of a Transformer
      piece-by-piece, indicating how each can be programmed. In the
      next section, we will actually build a network that classifies
      sentences as grammatical or not.
    </p>



    <a class="marker" href="#section-4.1" id="section-4.1"><span>4.1</span></a>
    <h2>Pointwise Dense Layers</h2>
    <p>As the next step, we will consider a pointwise dense layer,
      which requires one matrix parameter (the weights) and one vector
      parameter (the biases). Pointwise dense layers are used to
      compute the inputs to dot-product attention, as well as in the
      feed-forward layers of the Transformer.
    </p>
    <p>As a basic example, consider semes
      $$apple,banana,cherry,durian$$. Consider the dense layer with
      weights $$\{\{apple\rightarrow apple, -apple\rightarrow banana,
      -apple \rightarrow cherry, -banana \rightarrow apple,$$ $$banana
      \rightarrow banana, -banana \rightarrow cherry, -cherry
      \rightarrow apple, -cherry \rightarrow banana,$$ $$cherry
      \rightarrow cherry\}\}$$ and bias $$\langle\langle
      -durian\rangle\rangle$$. The following figure allows you to change
      both the parameters of the dense layer and the vectors that get
      run through it.
    </p>

    <d-figure id="figure-pointwise" style="border: 1px solid black">
      <svg id="figure-pointwise-svg" width="500" height="200" />
    </d-figure>
    <script>
      const figurePointwise = document.querySelector(
          "d-figure#figure-pointwise");
      const initTagPointwise = document.createElement("div");
      figurePointwise.appendChild(initTagPointwise);

      figurePointwise.addEventListener("ready", function () {
          const initTagPointwise = figurePointwise.querySelector("div");
          let layer = new HandDense(
              {program: `semes: apple banana cherry durian
mat: apple>apple, -apple>banana, -apple>cherry, -banana>apple,
     banana>banana, -banana>cherry, -cherry>apple, -cherry>banana,
     cherry>cherry
bias: -durian
examples:
    - ''
    - +apple
    - +banana
    - +cherry
    - +apple +banana
    - +apple -banana
    - +banana +cherry
    - +banana -cherry
    - +apple +cherry
    - -apple +cherry
    - +apple +banana +cherry
    - +apple +banana +cherry +durian
`,
               id: 'figure-pointwise',
               topLevel: true,
               document: document,
               container: initTagPointwise});
      });

    </script>



    <a class="marker" href="#section-4.2" id="section-4.2"><span>4.2</span></a>
    <h2>Word Embeddings</h2>
    <p>
      Our ultimate goal is to write down all the weights for a
      Transformer model that can perform a natural language task. We
      next discuss word embeddings. For simplicity, we omit the
      intermediate step of associating each word to an integer
      identifier, and simply map each word directly to a vector.
    </p>
    <table>
      <thead>
        <tr>
          <th>Word</th>
          <th>Embedding</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>I</td>
          <td>$$\langle\langle +nom +sg +1st +pro\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>you</td>
          <td>$$\langle\langle +nom +sg +2nd +pro\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>he</td>
          <td>$$\langle\langle +masc +nom +sg +3rd +pro\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>she</td>
          <td>$$\langle\langle +fem +nom +sg +3rd +pro\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>it</td>
          <td>$$\langle\langle +neut +sg +3rd +pro +expletive\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>me</td>
          <td>$$\langle\langle +acc +sg +1st +pro\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>you</td>
          <td>$$\langle\langle +sg +pl +2nd +pro\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>him</td>
          <td>$$\langle\langle +masc +acc +sg +3rd +pro\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>her</td>
          <td>$$\langle\langle +fem +acc +sg +3rd +pro\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>we</td>
          <td>$$\langle\langle +nom +pl +1st +pro\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>they</td>
          <td>$$\langle\langle +nom +pl +3rd +pro\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>us</td>
          <td>$$\langle\langle +acc +pl +1st +pro\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>them</td>
          <td>$$\langle\langle +acc +pl +3rd +pro\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>my</td>
          <td>$$\langle\langle +gen +sg +1st +pro\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>our</td>
          <td>$$\langle\langle +gen +pl +1st +pro\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>his</td>
          <td>$$\langle\langle +masc +gen +sg +3rd +pro\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>her</td>
          <td>$$\langle\langle +fem +gen +acc +sg +3rd +pro\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>its</td>
          <td>$$\langle\langle +neut +gen +sg +3rd +pro\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>their</td>
          <td>$$\langle\langle +gen +pl +3rd +pro\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>meet</td>
          <td>$$\langle \langle+meet +verb
            +plain +agentlack
            +patientlack\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>meets</td>
          <td>$$\langle \langle +meet +verb
            +3rdsg +agentlack
            +patientlack \rangle\rangle $$</td>
        </tr>
        <tr>
          <td>met</td>
          <td>$$\langle\langle +meet +verb
            +preterite +agentlack
            +patientlack\rangle\rangle$$</td>
        </tr>
        <tr>
          <td>pig</td>
          <td>$$\langle\langle +pig +noun +sg\rangle\rangle$$</td>
        <tr>
          <td>pigs</td>
          <td>$$\langle\langle +pig +noun +pl\rangle\rangle$$</td>
        </tr>

      </tbody>
    </table>
    <p>
      There are several things to note in these word embeddings.
      Firstly, a "content" word like "meet" or "pig" will generally
      have either itself or some other form of itself as one of its
      components<d-footnote>This happens primarily because we do not
        know of any completely interpretable encoding of semantic
        meanings, and we need some notion of semantics in order to
        perform some tasks, e.g. translation. Pretrained word embeddings
        are at least somewhat interpretable. We will revisit this point
        later when we talk about hybrid hand/learned
        approaches. </d-footnote>, while pronouns are fully specifiable
      in terms of various axes having to do with classic grammatical
      notions like case, gender, person, number, and so on. Secondly,
      content words come with a few extra <b>syntactic</b> components
      describing how the particular form of the word expresses
      grammatical notions like person (for verbs) and number (for both
      verbs and nouns). For instance, "pig" is singular
      $$\langle\langle +sg\rangle\rangle$$, while "pigs" is plural
      $$\langle\langle +pl \rangle\rangle$$.
    </p>
    <p>We describe the intended use of some the semes we use here:
    </p>
    <table>
      <tr>
        <th>Type</th>
        <th>Seme</th>
        <th>Meaning</th>
        <th>Example</th>
      </tr>
      <tr>
        <td style="vertical-align: top">Weirdness</td>
        <td>$$weird$$</td>
        <td>Some linguistic expectation has been violated</td>
        <td>I are</td>
      </tr>
      <tr>
        <td rowspan="2" style="vertical-align: top">Number</td>
        <td>$$sg$$</td>
        <td>Singular</td>
        <td>dog</td>
      </tr>
      <tr>
        <td>$$pl$$</td>
        <td>Plural</td>
        <td>dogs</td>
      </tr>
      <tr>
        <td rowspan="3" style="vertical-align: top">Case</td>
        <td>$$nom$$</td>
        <td>Nominative case</td>
        <td>I/they/she/he</td>
      </tr>
      <tr>
        <td>$$acc$$</td>
        <td>Accusative case</td>
        <td>me/them/her/him</td>
      </tr>
      <tr>
        <td>$$gen$$</td>
        <td>Genitive case</td>
        <td>my/their/her/his</td>
      </tr>
      <tr>
        <td rowspan="3" style="vertical-align: top">Person</td>
        <td>$$1st$$</td>
        <td>1st person</td>
        <td>I/me/mine/myself</td>
      </tr>
      <tr>
        <td>$$2nd$$</td>
        <td>2nd person</td>
        <td>you/your/yourself</td>
      </tr>
      <tr>
        <td>$$3rd$$</td>
        <td>3rd person</td>
        <td>she/her/he/him/his/they/them/their</td>
      </tr>
      <tr>
        <td rowspan="3" style="vertical-align: top">Role</td>
        <td>$$agent$$</td>
        <td>One who performs an action</td>
        <td><b>She</b> threw the ball</td>
      </tr>
      <tr>
        <td>$$experiencer$$</td>
        <td>One who experiences some perception</td>
        <td><b>He</b> saw a dog</td>
      </tr>
      <tr>
        <td>$$percept$$</td>
        <td>Something that is perceived</td>
        <td>He saw a <b>dog</b></td>
      </tr>
      <tr>
        <td rowspan="3" style="vertical-align: top">Role Requirement</td>
        <td>$$agentlack$$</td>
        <td>Used for verbs that require an agent</td>
        <td>She <b>threw</b> the ball</td>
      </tr>
      <tr>
        <td>$$experiencerlack$$</td>
        <td>Used for verbs that require an experiencer</td>
        <td>He <b>saw</b> a dog</td>
      </tr>
      <tr>
        <td>$$perceptposs$$</td>
        <td>Used for verbs that can take but do not require a
          percept</td>
        <td>He <b>saw</b> a dog / He <b>saw</b></td>
      </tr>
    </table>



    <a class="marker" href="#section-4.3" id="section-4.3"><span>4.3</span></a>
    <h2>Transformer Feed-Forward Layers</h2>
    <p>Next we consider the feed-forward layers of Transformer. These
      are typically two dense layers with a ReLU nonlinearity between
      them. The intermediate dimension (referred to as the "filter
      size") is usually larger (typically by a factor of four) than the
      hidden size of the network.
    </p>

    <p>One of the primary uses we have found for the Transformer
    feed-forward layer is to allow us to reason about logical
    conjunctions ($$a$$ AND $$b$$) and logical disjunctions ($$a$$ OR
    $$b$$). A single dense layer does not have the representational
    capacity to represent either of these notions in a satisfactory
    manner. In a Transformer feed-forward layer, we can represent
    $$a$$ AND $$b$$ as $$f_{a\, \mathrm{AND}\, b}(v) =
    \mathrm{ReLU}(v\cdot \{\{a\rightarrow x, b\rightarrow x\}\} -
    \langle\langle x \rangle\rangle)$$. Here we can read the value of
    $$a$$ AND $$b$$ out from the coefficient of $$x$$ in
    $$f(v)$$. Similarly we can represent $$a$$ OR $$b$$ as $$f_{a\,
    \mathrm{OR}\, b}(v) =v\cdot \{\{ +a\rightarrow x, +b\rightarrow
    x\}\} - f_{a\, \mathrm{AND}\, b}(v)$$ $$=v \cdot \{\{
    +a\rightarrow x, +b\rightarrow x\}\} -\mathrm{ReLU}(v\cdot
    \{\{a\rightarrow x, b\rightarrow x\}\} - \langle\langle x
    \rangle\rangle)$$.  We now present editable code. The provided
    code maps $$apple$$ OR $$banana$$ to $$yum$$ and $$cherry$$ AND
    $$durian$$ to $$yuck$$.<d-footnote>No offense to lovers of either
    fruit.</d-footnote> As a challenge, consider how you would
    represent $$apple$$ OR $$banana$$ OR $$cherry$$. (You may find
    you need auxilliary semes!)
    </p>


    <d-figure id="figure-feedforward" style="border: 1px solid black">
      <svg id="figure-feedforward-svg" width="500" height="200" />
    </d-figure>
    <script>
      const figureFeedForward = document.querySelector(
          "d-figure#figure-feedforward");
      const initTagFeedForward = document.createElement("div");
      figureFeedForward.appendChild(initTagFeedForward);

      figureFeedForward.addEventListener("ready", function () {
          const initTagFeedForward = figureFeedForward.querySelector("div");
          let layer = new HandFeedForward(
              {program: `semes: apple banana cherry durian yum yuck
mat1: apple>apple apple>yum banana>banana banana>yum cherry>yuck durian>yuck
bias1: -yum -yuck
mat2: apple>yum banana>yum -yum>yum yuck>yuck
bias2: ''
examples:
    - +apple
    - +banana
    - +cherry
    - +durian
    - +apple +banana
    - +banana +cherry
    - +apple +cherry
    - +apple +banana +cherry
    - +apple +durian
    - +banana +durian
    - +cherry +durian
    - +apple +banana +durian
    - +banana +cherry +durian
    - +apple +cherry +durian
    - +apple +banana +cherry +durian
`,
               id: "figure-feedforward",
               topLevel: true,
               document: document,
               container: initTagFeedForward});
      });

    </script>



    <a class="marker" href="#section-4.4" id="section-4.4"><span>4.4</span></a>
    <h2>Transformer Attention Layers</h2>
    <p>Finally, we come to the most iconic layer of the Transformer,
      the attention layer. This layer is the only layer in which the
      representations of different words interact.
    </p>
    <p>For now, we deal only with a single attention head for
      simplicity. Later, we will consider multi-head attention. For
      the time being, we are not using positional embeddings, which
      greatly restricts the ability of the network to associate nouns
      with the correct modifiers (and you will see such errors in the
      output of the next figure). We will address this shortcoming
      later, once we have some slightly better notation.
    </p>

    <d-figure id="figure-attention" style="border: 1px solid black">
      <svg id="figure-attention-svg" width="500" height="200" />
    </d-figure>
    <script>
      const figureAttention = document.querySelector(
          "d-figure#figure-attention");
      const initTagAttention = document.createElement("div");
      figureAttention.appendChild(initTagAttention);

      figureAttention.addEventListener("ready", function () {
          const initTagAttention = figureAttention.querySelector("div");
          let layer = new HandSelfAttention(
              {program: `semes: red green blue apple banana cherry durian
       det adjective noun conjunction xa
keymat: 10adjective>xa 10det>det 10conjunction>conjunction 10det>adjective
querymat: 10noun>xa 10det>det 10conjunction>conjunction 10adjective>adjective
valuemat: red>red green>green blue>blue
examples:
    - # apple gets red
      - a: +det
      - red: +adjective +red
      - apple: +noun +apple
    - # banana gets green
      - a: +det
      - green: +adjective +green
      - banana: +noun +banana
    - # cherry gets blue
      - a: +det
      - blue: +adjective +blue
      - cherry: +noun +cherry
    - # both nouns get both colors, which is incorrect
      - a: +det
      - red: +adjective +red
      - cherry: +noun +cherry
      - and: +conjunction
      - a: +det
      - blue: +adjective +blue
      - durian: +noun +durian
`,
               id: "figure-attention",
               topLevel: true,
               document: document,
               container: initTagAttention});
      });

    </script>



    <a class="marker" href="#section-4.5" id="section-4.5"><span>4.5</span></a>
    <h2>Positional Encoding, Explained</h2>



    <a class="marker" href="#section-4.6" id="section-4.6"><span>4.6</span></a>
    <h2>Transformer Attention Layers with Positional Encoding</h2>



    <a class="marker" href="#section-5" id="section-5"><span>5</span></a>
    <h2>Grammaticality Transformer</h2>

    <p>
      In this section, we show a Transformer programmed to make
      grammaticality judgments in English.
    </p>

    <d-figure id="figure-grammaticality" style="border: 1px solid black">
      <svg id="figure-grammaticality-svg" width="500" height="200" />
    </d-figure>
    <script>
      const figureGrammaticality = document.querySelector(
          "d-figure#figure-grammaticality");
      const initTagGrammaticality = document.createElement("div");
      figureGrammaticality.appendChild(initTagGrammaticality);

//       figureGrammaticality.addEventListener("ready", function () {
//           const initTagGrammaticality = figureGrammaticality.querySelector("div");
//           let layer = new HandTransformer(
//               {program: `# English Grammaticality Classifier
// semes:
// examples:
//     - she will eat a very small red apple
// `,
//                id: "figure-grammaticality",
//                topLevel: true,
//                document: document,
//                container: initTagGrammaticality});
//       });

    </script>



    <a class="marker" href="#section-6" id="section-6"><span>6</span></a>
    <h2>Translation Transformer</h2>

    <p>In this section, we show a Transformer programmed to translate
      English to French. This network is solving a toy version of the
      problem; the author has very little knowledge of French,
      unfortunately.
    </p>

    <d-figure id="figure-translation" style="border: 1px solid black">
      <svg id="figure-translation-svg" width="500" height="200" />
    </d-figure>
    <script>
      const figureTranslation = document.querySelector(
          "d-figure#figure-translation");
      const initTagTranslation = document.createElement("div");
      figureTranslation.appendChild(initTagTranslation);

      figureTranslation.addEventListener("ready", function () {
          const initTagTranslation = figureTranslation.querySelector("div");
          let layer = new HandTransformer(
              {program: `# English-to-French (Toy) Translation Transformer

semes: subject mainverb dirobject indirobject first second third pro
       noun verb adjective article male female shirt coat donut steak
       salad apple human cat dog red green blue wearable edible eating
       wearing weird filler sos eos elided helper xa xb xc xd xe xf xg
       xh xi xj xk xl

encoder_lexicon:
    sos: +sos
    eos: +eos
    I: +noun +pro +first
    he: +male +noun +pro +third
    she: +female +noun +pro +third
    ate: +eating +verb
    wore: +wearing +verb
    a: +article
    an: +article
    edible: +edible +adjective
    wearable: +wearable +adjective
    red: +red +adjective
    green: +green +adjective
    blue: +blue +adjective
    shirt: +wearable +shirt +noun
    coat: +wearable +coat +noun
    apple: +edible +apple +noun
    donut: +edible +donut +noun
    steak: +edible +steak +noun
    salad: +edible +salad +noun
    human: +human +noun
    cat: +cat +noun
    dog: +dog +noun

decoder_lexicon:
    sos: +sos
    eos: +eos
    'je': +subject +first -elided +pro
    'j\`': +subject +first +elided +pro
    'il': +subject +male +third +pro
    'elle': +subject +female +third +pro
    'ai': +verb +helper +first
    'a': +verb +helper +third
    'mangé': +eating +verb +first +third
    'portais': +wearing +verb +first
    'portait': +wearing +verb +third
    'un': +article +coat +dog +human +donut +steak
    'une': +article +shirt +cat +apple +salad
    'comestible': +edible +adjective
    'portable': +wearable +adjective
    'rouge': +red +adjective
    'vert': +green +adjective
    'bleu': +blue +adjective
    'chemise': +shirt +noun
    'manteau': +coat +noun
    'pomme': +apple +noun
    'beignet': +donut +noun
    'steak': +steak +noun
    'salade': +salad +noun
    'humain': +human +noun
    'chat': +cat +noun
    'chien': +dog +noun


architecture:
    encoder:
        - self_attention1
    decoder:
        - self_attention2
        - encdec_attention1
        - feedforward2

self_attention1:
    type: HandMultiheadSelfAttention
    heads:
        - docstring: modification
          querypos: +1
          keypos: 0
          querymat: adjective>xa
          keymat: noun>xa -verb>xa
          valuemat: red>red green>green blue>blue edible>edible
                    wearable>wearable
        - docstring: role identification layer (kind of cheating here)
          querymat: pro>xa noun>xb verb>xc
          keymat: pro>xa noun>xb verb>xc
          valuemat: pro>subject noun>dirobject verb>mainverb

self_attention2:
    type: HandMultiheadSelfAttention
    heads:
        - docstring: erase the embedding of the input word and shift
                     towards next word

          querymat: sos>sos eos>eos first>first second>second
                    third>third pro>pro noun>noun verb>verb
                    adjective>adjective article>article male>male
                    female>female shirt>shirt coat>coat donut>donut
                    steak>steak salad>salad apple>apple human>human
                    cat>cat dog>dog red>red green>green blue>blue
                    wearable>wearable edible>edible eating>eating
                    wearing>wearing

          keymat: sos>sos eos>eos first>first second>second
                  third>third pro>pro noun>noun verb>verb
                  adjective>adjective article>article male>male
                  female>female shirt>shirt coat>coat donut>donut
                  steak>steak salad>salad apple>apple human>human
                  cat>cat dog>dog red>red green>green blue>blue
                  wearable>wearable edible>edible eating>eating
                  wearing>wearing

          valuemat: sos>subject pro>verb pro>helper -pro>adjective
                    2helper>verb 3helper>eating verb>article article>noun
                    noun>adjective 2adjective>eos -noun>verb -2sos>sos
                    -2eos>eos -2first>first -2second>second
                    -2third>third -20pro>pro -20noun>noun
                    -20helper>helper -20adjective>adjective
                    -2article>article -2male>male -2female>female
                    -2shirt>shirt -2coat>coat -2donut>donut
                    -2steak>steak -2salad>salad -2apple>apple
                    -2human>human -2cat>cat -2dog>dog -2red>red
                    -2green>green -2blue>blue -2wearable>wearable
                    -2edible>edible -20eating>eating -20wearing>wearing
                    -2subject>subject 

encdec_attention1:
    type: HandMultiheadEncdecAttention
    heads:
        - docstring: most stuff
          querymat: subject>xa verb>xb dirobject>xc noun>xc adjective>xd
          keymat: subject>xa verb>xb dirobject>xc noun>xc adjective>xd
          valuemat: first>first second>second third>third pro>pro
                    noun>noun verb>verb adjective>adjective
                    article>article male>male female>female
                    shirt>shirt coat>coat donut>donut steak>steak
                    salad>salad apple>apple human>human cat>cat
                    dog>dog red>red green>green blue>blue
                    wearable>wearable edible>edible eating>eating
                    wearing>wearing

        - docstring: article gender
          querymat: article>xa
          keymat: dirobject>xa noun>xa
          valuemat: shirt>shirt coat>coat human>human cat>cat dog>dog
                    donut>donut salad>salad steak>steak apple>apple

        - docstring: helper verb
          querymat: verb>xa
          keymat: verb>xa
          valuemat: eating>helper -6wearing>helper

        - docstring: which verb
          querymat: helper>xa
          keymat: pro>xa
          valuemat: first>first third>third

        - docstring: subject-verb agreement
          querymat: verb>xa helper>xa
          keymat: pro>xa
          valuemat: first>first third>third

        - docstring: whether to elide the e in je
          querymat: subject>xa
          keymat: verb>xa
          valuemat: eating>elided -wearing>elided

feedforward2:
    type: HandFeedForward
    docstring: honestly do not remember writing this one
    mat1: adjective>xa -red>xa -blue>xa -green>xa -wearable>xa
          -edible>xa edible>xb wearable>xc human>xb human>xc
          article>article
    bias1: -xb -xc
    mat2: -article>verb -article>noun -article>pro
          2article>article -2xb>pro -xa>adjective xa>eos
    bias2: ''

examples:
    - he ate a blue apple
    - he ate a dog
    - he ate a green human
    - he ate a steak
    - he ate a wearable salad
    - he ate an edible donut
    - he wore a blue shirt
    - he wore a blue steak
    - he wore a green steak
    - he wore a human
    - he wore a wearable cat
    - he wore an edible steak
    - I ate a blue human
    - I ate a red salad
    - I ate a wearable donut
    - I wore a blue human
    - I wore a coat
    - I wore a green donut
    - I wore a red apple
    - I wore a shirt
    - I wore a wearable shirt
    - I wore an edible shirt
    - she ate a blue shirt
    - she ate a green dog
    - she ate a red dog
    - she ate a wearable cat
    - she ate an edible dog
    - she wore a blue salad
    - she wore a green salad
    - she wore a steak
    - she wore a wearable dog
    - she wore an apple
    - she wore an edible salad
`,
               id: "figure-translation",
               topLevel: true,
               document: document,
               container: initTagTranslation});
      });

    </script>



    <a class="marker" href="#section-9" id="section-9"><span>9</span></a>
    <h2>Unused Stuff</h2>
    <p>As an example, consider the problem of classifying sentences
      as being sensible (e.g., "He saw the dog") or anomalous (e.g.,
      "The apple saw the dog"). There are many ways for a sentence to be
      anomalous, of course, and we cannot identify them all with a
      single feed-forward layer. So here we will limit ourselves to a
      layer that can identify what is anomalous about the sentence "The
      apple saw the dog", given that preceding layers have already done
      the work of analyzing the syntactic structure of the sentence.
    </p>



  </d-article>

  <d-appendix>

    <h3>Reviewers</h3>
    <p>Some text with links describing who reviewed the article.</p>

    <d-bibliography src="bibliography.bib"></d-bibliography>
  </d-appendix>

  <distill-footer></distill-footer>

</body>
